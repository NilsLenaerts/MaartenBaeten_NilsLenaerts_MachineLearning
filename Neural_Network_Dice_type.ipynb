{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 16:46:25.201786: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-27 16:46:26.443978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-27 16:46:26.444162: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-27 16:46:26.444171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\maart\\\\Documents\\\\IIW\\\\Machine\\\\Taak\\\\Data\\\\dice-d4-d6-d8-d10-d12-d20\\\\dice\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 138\u001b[0m\n\u001b[1;32m    136\u001b[0m     probability_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential([model, tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mSoftmax()])\n\u001b[1;32m    137\u001b[0m     probability_model(x_test[:\u001b[39m5\u001b[39m])\n\u001b[0;32m--> 138\u001b[0m main()\n",
      "Cell \u001b[0;32mIn [1], line 89\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m     87\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTensorFlow version:\u001b[39m\u001b[39m\"\u001b[39m, tf\u001b[39m.\u001b[39m__version__)\n\u001b[0;32m---> 89\u001b[0m     gray_arrays_train, labels_int_train,j_train \u001b[39m=\u001b[39m loadData(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mmaart\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mIIW\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mMachine\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mTaak\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mData\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdice-d4-d6-d8-d10-d12-d20\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdice\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     91\u001b[0m     x_train \u001b[39m=\u001b[39m gray_arrays_train\n\u001b[1;32m     92\u001b[0m     y_train \u001b[39m=\u001b[39m labels_int_train\n",
      "Cell \u001b[0;32mIn [1], line 49\u001b[0m, in \u001b[0;36mloadData\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     48\u001b[0m j \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 49\u001b[0m \u001b[39mfor\u001b[39;00m directory \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(directory_path):\n\u001b[1;32m     50\u001b[0m     newPath \u001b[39m=\u001b[39m directory_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m directory\n\u001b[1;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(newPath):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\maart\\\\Documents\\\\IIW\\\\Machine\\\\Taak\\\\Data\\\\dice-d4-d6-d8-d10-d12-d20\\\\dice\\\\train'"
     ]
    }
   ],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.cm as cm \n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "from scipy import misc\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils\n",
    "\n",
    "# importing os module \n",
    "import os \n",
    "\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "X_size = 14284 #2046 validation 14284 train 2039 valid which are 691200 and 14284 which are 691200 \n",
    "Gray_size = 57600  #230400\n",
    "\n",
    "def loadData(directory_path):\n",
    "    labels_int = np.zeros((X_size))\n",
    "    gray_arrays = np.zeros((X_size,Gray_size))\n",
    "    directory_path\n",
    "    ext = ('.jpg')\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for directory in os.listdir(directory_path):\n",
    "        newPath = directory_path + '\\\\' + directory\n",
    "        for file in os.listdir(newPath):\n",
    "            if file.endswith(ext):\n",
    "                path = newPath + '\\\\' + file\n",
    "\n",
    "                img = io.imread(path)\n",
    "                if(img.size == 691200):\n",
    "                    imgGray = color.rgb2gray(img)\n",
    "                    res_img = rescale(imgGray, 0.5, anti_aliasing=False)\n",
    "                    imgn = np.reshape(res_img,(1,Gray_size),order='F')\n",
    "                    gray_arrays[i] = imgn\n",
    "                    type = directory\n",
    "                    match type:\n",
    "                        case 'd4':\n",
    "                            labels_int[i]=0\n",
    "                        case 'd6':\n",
    "                            labels_int[i]=1\n",
    "                        case 'd8':\n",
    "                            labels_int[i]=2\n",
    "                        case 'd10':\n",
    "                            labels_int[i]=3\n",
    "                        case 'd12':\n",
    "                            labels_int[i]=4\n",
    "                        case 'd20':\n",
    "                            labels_int[i]=5   \n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                i = i + 1   \n",
    "                j=j+1                                                                                                                \n",
    "            else:\n",
    "                continue\n",
    "    i = 0\n",
    "\n",
    "    return gray_arrays, labels_int,j\n",
    "  \n",
    "def main():\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "    gray_arrays_train, labels_int_train,j_train = loadData(r\"C:\\Users\\maart\\Documents\\IIW\\Machine\\Taak\\Data\\dice-d4-d6-d8-d10-d12-d20\\dice\\train\")\n",
    "\n",
    "    x_train = gray_arrays_train\n",
    "    y_train = labels_int_train\n",
    "\n",
    "    gray_arrays_test, labels_int_test, j_test = loadData(r\"C:\\Users\\maart\\Documents\\IIW\\Machine\\Taak\\Data\\dice-d4-d6-d8-d10-d12-d20\\dice\\valid\")\n",
    "    x_test = gray_arrays_test\n",
    "    y_test = labels_int_test\n",
    "\n",
    "    # Setup the parameters you will use for this exercise\n",
    "    input_layer_size  = 57600  # 240x240 Input Images \n",
    "    hidden_layer_size = 10000   # 200 hidden units\n",
    "    num_labels = 6          # 6 labels\n",
    "\n",
    "    # Build a tf.keras.Sequential model by stacking layers.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(57600,1)),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(num_labels)\n",
    "    ])\n",
    "\n",
    "    # For each example, the model returns a vector of logits or log-odds scores, one for each class.\n",
    "    predictions = model(x_train[:1]).numpy()\n",
    "\n",
    "    # The tf.nn.softmax function converts these logits to probabilities for each class:\n",
    "    tf.nn.softmax(predictions).numpy()\n",
    "\n",
    "    # Define a loss function for training using losses.SparseCategoricalCrossentropy, \n",
    "    # which takes a vector of logits and a True index and returns a scalar loss for each example.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # This loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n",
    "    loss_fn(y_train[:1], predictions).numpy()\n",
    "\n",
    "    # Before you start training, configure and compile the model using Keras Model.compile. \n",
    "    # Set the optimizer class to adam, set the loss to the loss_fn function you defined earlier\n",
    "    # and specify a metric to be evaluated for the model by setting the metrics parameter to accuracy.\n",
    "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    # Use the Model.fit method to adjust your model parameters and minimize the loss:\n",
    "    model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "    #The Model.evaluate method checks the models performance, usually on a \"Validation-set\" or \"Test-set\".\n",
    "    model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "    # If you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:\n",
    "    probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "    probability_model(x_test[:5])\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
